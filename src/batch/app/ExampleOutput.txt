Spark Data Output:

Item pair: (1, 3) , count: 3
{"recommended items": "3", "status": "Created"}
{"recommended items": "1", "status": "Created"}
Item pair: (3, 5) , count: 3
{"recommended items": "1,5", "status": "Updated"}
{"recommended items": "3", "status": "Created"}
Item pair: (1, 2) , count: 3
{"recommended items": "3,2", "status": "Updated"}
{"recommended items": "1", "status": "Created"}
Item pair: (2, 3) , count: 3
{"recommended items": "1,3", "status": "Updated"}
{"recommended items": "1,5,2", "status": "Updated"}
Item pair: (5, 6) , count: 3
{"recommended items": "3,6", "status": "Updated"}
{"recommended items": "5", "status": "Created"}
Item pair: (3, 4) , count: 4
{"recommended items": "1,5,2,4", "status": "Updated"}
{"recommended items": "3", "status": "Created"}
Item pair: (1, 4) , count: 3
{"recommended items": "3,2,4", "status": "Updated"}
{"recommended items": "3,1", "status": "Updated"}


Full Log:

2020-05-08 23:34:33,976 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-08 23:34:35,262 INFO spark.SparkContext: Running Spark version 2.4.1
2020-05-08 23:34:35,376 INFO spark.SparkContext: Submitted application: PopularItems
2020-05-08 23:34:35,567 INFO spark.SecurityManager: Changing view acls to: root
2020-05-08 23:34:35,568 INFO spark.SecurityManager: Changing modify acls to: root
2020-05-08 23:34:35,568 INFO spark.SecurityManager: Changing view acls groups to: 
2020-05-08 23:34:35,569 INFO spark.SecurityManager: Changing modify acls groups to: 
2020-05-08 23:34:35,571 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2020-05-08 23:34:36,324 INFO util.Utils: Successfully started service 'sparkDriver' on port 46221.
2020-05-08 23:34:36,396 INFO spark.SparkEnv: Registering MapOutputTracker
2020-05-08 23:34:36,458 INFO spark.SparkEnv: Registering BlockManagerMaster
2020-05-08 23:34:36,470 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-08 23:34:36,472 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2020-05-08 23:34:36,497 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-dc8b51c1-4222-4124-8dfb-ee515e4c9ef4
2020-05-08 23:34:36,554 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
2020-05-08 23:34:36,600 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2020-05-08 23:34:36,814 INFO util.log: Logging initialized @6222ms
2020-05-08 23:34:37,053 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2020-05-08 23:34:37,093 INFO server.Server: Started @6503ms
2020-05-08 23:34:37,147 INFO server.AbstractConnector: Started ServerConnector@5905ffce{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-08 23:34:37,150 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2020-05-08 23:34:37,231 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7106175f{/jobs,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,233 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17f40074{/jobs/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,234 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a204791{/jobs/job,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,247 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@311ef3c8{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,248 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@497a36b5{/stages,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,249 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a7b52fe{/stages/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,251 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29f03d7a{/stages/stage,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,254 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@778d0752{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,255 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fbeff34{/stages/pool,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,257 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ef81f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,259 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7dad612b{/storage,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,261 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62055e5b{/storage/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,270 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4aea5a70{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,272 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d539d32{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,274 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b61078a{/environment,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,275 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@635e633b{/environment/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,277 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b7f8cfa{/executors,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,279 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a6434aa{/executors/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,280 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@799a253b{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,281 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@694dd688{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,308 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ed28f07{/static,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,320 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@539357d4{/,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,334 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20acaf5{/api,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,351 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fb67aac{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,357 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d07162f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-08 23:34:37,367 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-master:4040
2020-05-08 23:34:37,731 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
2020-05-08 23:34:37,951 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.20.0.4:7077 after 135 ms (0 ms spent in bootstraps)
2020-05-08 23:34:38,342 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200508233438-0001
2020-05-08 23:34:38,470 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36429.
2020-05-08 23:34:38,500 INFO netty.NettyBlockTransferService: Server created on spark-master:36429
2020-05-08 23:34:38,529 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-08 23:34:38,526 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200508233438-0001/0 on worker-20200508232257-172.20.0.6-8881 (172.20.0.6:8881) with 2 core(s)
2020-05-08 23:34:38,550 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200508233438-0001/0 on hostPort 172.20.0.6:8881 with 2 core(s), 512.0 MB RAM
2020-05-08 23:34:38,662 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200508233438-0001/0 is now RUNNING
2020-05-08 23:34:38,783 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-master, 36429, None)
2020-05-08 23:34:38,815 INFO storage.BlockManagerMasterEndpoint: Registering block manager spark-master:36429 with 366.3 MB RAM, BlockManagerId(driver, spark-master, 36429, None)
2020-05-08 23:34:38,832 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-master, 36429, None)
2020-05-08 23:34:38,838 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-master, 36429, None)
2020-05-08 23:34:39,955 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c2b4da6{/metrics/json,null,AVAILABLE,@Spark}
2020-05-08 23:34:40,055 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
{"found": true, "deleted": true}
2020-05-08 23:34:42,821 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 420.1 KB, free 365.9 MB)
2020-05-08 23:34:43,108 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.1 KB, free 365.9 MB)
2020-05-08 23:34:43,127 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-master:36429 (size: 37.1 KB, free: 366.3 MB)
2020-05-08 23:34:43,167 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
2020-05-08 23:34:43,878 INFO mapred.FileInputFormat: Total input files to process : 1
2020-05-08 23:34:44,579 INFO spark.SparkContext: Starting job: collect at /app/spark_script.py:30
2020-05-08 23:34:44,712 INFO scheduler.DAGScheduler: Registering RDD 3 (distinct at /app/spark_script.py:20)
2020-05-08 23:34:44,726 INFO scheduler.DAGScheduler: Registering RDD 7 (groupByKey at /app/spark_script.py:22)
2020-05-08 23:34:44,739 INFO scheduler.DAGScheduler: Registering RDD 11 (groupByKey at /app/spark_script.py:24)
2020-05-08 23:34:44,758 INFO scheduler.DAGScheduler: Got job 0 (collect at /app/spark_script.py:30) with 2 output partitions
2020-05-08 23:34:44,762 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at /app/spark_script.py:30)
2020-05-08 23:34:44,769 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
2020-05-08 23:34:44,784 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)
2020-05-08 23:34:44,823 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /app/spark_script.py:20), which has no missing parents
2020-05-08 23:34:45,084 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.3 KB, free 365.8 MB)
2020-05-08 23:34:45,134 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KB, free 365.8 MB)
2020-05-08 23:34:45,145 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-master:36429 (size: 7.2 KB, free: 366.3 MB)
2020-05-08 23:34:45,168 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2020-05-08 23:34:45,265 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /app/spark_script.py:20) (first 15 tasks are for partitions Vector(0, 1))
2020-05-08 23:34:45,275 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
2020-05-08 23:34:47,182 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.6:51382) with ID 0
2020-05-08 23:34:47,276 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.20.0.6, executor 0, partition 0, PROCESS_LOCAL, 7873 bytes)
2020-05-08 23:34:47,303 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 172.20.0.6, executor 0, partition 1, PROCESS_LOCAL, 7873 bytes)
2020-05-08 23:34:47,695 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.20.0.6:36197 with 93.3 MB RAM, BlockManagerId(0, 172.20.0.6, 36197, None)
2020-05-08 23:34:48,568 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.0.6:36197 (size: 7.2 KB, free: 93.3 MB)
2020-05-08 23:34:49,061 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.6:36197 (size: 37.1 KB, free: 93.3 MB)
2020-05-08 23:34:51,997 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4687 ms on 172.20.0.6 (executor 0) (1/2)
2020-05-08 23:34:52,019 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4795 ms on 172.20.0.6 (executor 0) (2/2)
2020-05-08 23:34:52,023 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-08 23:34:52,036 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51515
2020-05-08 23:34:52,098 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (distinct at /app/spark_script.py:20) finished in 7.095 s
2020-05-08 23:34:52,113 INFO scheduler.DAGScheduler: looking for newly runnable stages
2020-05-08 23:34:52,116 INFO scheduler.DAGScheduler: running: Set()
2020-05-08 23:34:52,124 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ShuffleMapStage 2, ResultStage 3)
2020-05-08 23:34:52,136 INFO scheduler.DAGScheduler: failed: Set()
2020-05-08 23:34:52,163 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[7] at groupByKey at /app/spark_script.py:22), which has no missing parents
2020-05-08 23:34:52,193 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.3 KB, free 365.8 MB)
2020-05-08 23:34:52,204 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.3 KB, free 365.8 MB)
2020-05-08 23:34:52,212 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-master:36429 (size: 7.3 KB, free: 366.2 MB)
2020-05-08 23:34:52,216 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2020-05-08 23:34:52,220 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[7] at groupByKey at /app/spark_script.py:22) (first 15 tasks are for partitions Vector(0, 1))
2020-05-08 23:34:52,220 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
2020-05-08 23:34:52,240 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 172.20.0.6, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
2020-05-08 23:34:52,247 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 172.20.0.6, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
2020-05-08 23:34:52,422 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.20.0.6:36197 (size: 7.3 KB, free: 93.2 MB)
2020-05-08 23:34:52,499 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.20.0.6:51382
2020-05-08 23:34:52,691 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 451 ms on 172.20.0.6 (executor 0) (1/2)
2020-05-08 23:34:52,737 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 502 ms on 172.20.0.6 (executor 0) (2/2)
2020-05-08 23:34:52,745 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (groupByKey at /app/spark_script.py:22) finished in 0.553 s
2020-05-08 23:34:52,746 INFO scheduler.DAGScheduler: looking for newly runnable stages
2020-05-08 23:34:52,746 INFO scheduler.DAGScheduler: running: Set()
2020-05-08 23:34:52,748 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
2020-05-08 23:34:52,748 INFO scheduler.DAGScheduler: failed: Set()
2020-05-08 23:34:52,751 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /app/spark_script.py:24), which has no missing parents
2020-05-08 23:34:52,754 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-08 23:34:52,762 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KB, free 365.8 MB)
2020-05-08 23:34:53,172 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 365.8 MB)
2020-05-08 23:34:53,175 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-master:36429 (size: 8.0 KB, free: 366.2 MB)
2020-05-08 23:34:53,181 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2020-05-08 23:34:53,193 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /app/spark_script.py:24) (first 15 tasks are for partitions Vector(0, 1))
2020-05-08 23:34:53,193 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
2020-05-08 23:34:53,222 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, 172.20.0.6, executor 0, partition 0, NODE_LOCAL, 7655 bytes)
2020-05-08 23:34:53,250 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, 172.20.0.6, executor 0, partition 1, NODE_LOCAL, 7655 bytes)
2020-05-08 23:34:53,325 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on spark-master:36429 in memory (size: 7.2 KB, free: 366.2 MB)
2020-05-08 23:34:53,360 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.20.0.6:36197 in memory (size: 7.2 KB, free: 93.3 MB)
2020-05-08 23:34:53,378 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.20.0.6:36197 (size: 8.0 KB, free: 93.2 MB)
2020-05-08 23:34:53,429 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.20.0.6:51382
2020-05-08 23:34:53,618 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 369 ms on 172.20.0.6 (executor 0) (1/2)
2020-05-08 23:34:53,652 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 430 ms on 172.20.0.6 (executor 0) (2/2)
2020-05-08 23:34:53,654 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-08 23:34:53,658 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (groupByKey at /app/spark_script.py:24) finished in 0.902 s
2020-05-08 23:34:53,659 INFO scheduler.DAGScheduler: looking for newly runnable stages
2020-05-08 23:34:53,660 INFO scheduler.DAGScheduler: running: Set()
2020-05-08 23:34:53,660 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)
2020-05-08 23:34:53,662 INFO scheduler.DAGScheduler: failed: Set()
2020-05-08 23:34:53,666 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[14] at collect at /app/spark_script.py:30), which has no missing parents
2020-05-08 23:34:53,677 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.5 KB, free 365.8 MB)
2020-05-08 23:34:53,680 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KB, free 365.8 MB)
2020-05-08 23:34:53,682 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-master:36429 (size: 6.0 KB, free: 366.2 MB)
2020-05-08 23:34:53,688 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2020-05-08 23:34:53,692 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[14] at collect at /app/spark_script.py:30) (first 15 tasks are for partitions Vector(0, 1))
2020-05-08 23:34:53,692 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
2020-05-08 23:34:53,697 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 172.20.0.6, executor 0, partition 0, NODE_LOCAL, 7666 bytes)
2020-05-08 23:34:53,697 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 172.20.0.6, executor 0, partition 1, NODE_LOCAL, 7666 bytes)
2020-05-08 23:34:53,745 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.20.0.6:36197 (size: 6.0 KB, free: 93.2 MB)
2020-05-08 23:34:53,783 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.0.6:51382
2020-05-08 23:34:53,869 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 175 ms on 172.20.0.6 (executor 0) (1/2)
2020-05-08 23:34:53,896 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 199 ms on 172.20.0.6 (executor 0) (2/2)
2020-05-08 23:34:53,900 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
2020-05-08 23:34:53,904 INFO scheduler.DAGScheduler: ResultStage 3 (collect at /app/spark_script.py:30) finished in 0.230 s
2020-05-08 23:34:53,944 INFO scheduler.DAGScheduler: Job 0 finished: collect at /app/spark_script.py:30, took 9.363901 s
Item pair: (1, 3) , count: 3
{"recommended items": "3", "status": "Created"}
{"recommended items": "1", "status": "Created"}
Item pair: (3, 5) , count: 3
{"recommended items": "1,5", "status": "Updated"}
{"recommended items": "3", "status": "Created"}
Item pair: (1, 2) , count: 3
{"recommended items": "3,2", "status": "Updated"}
{"recommended items": "1", "status": "Created"}
Item pair: (2, 3) , count: 3
{"recommended items": "1,3", "status": "Updated"}
{"recommended items": "1,5,2", "status": "Updated"}
Item pair: (5, 6) , count: 3
{"recommended items": "3,6", "status": "Updated"}
{"recommended items": "5", "status": "Created"}
Item pair: (3, 4) , count: 4
{"recommended items": "1,5,2,4", "status": "Updated"}
{"recommended items": "3", "status": "Created"}
Item pair: (1, 4) , count: 3
{"recommended items": "3,2,4", "status": "Updated"}
{"recommended items": "3,1", "status": "Updated"}
Popular items done
2020-05-08 23:34:54,667 INFO server.AbstractConnector: Stopped Spark@5905ffce{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-08 23:34:54,686 INFO ui.SparkUI: Stopped Spark web UI at http://spark-master:4040
2020-05-08 23:34:54,712 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
2020-05-08 23:34:54,714 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2020-05-08 23:34:54,820 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2020-05-08 23:34:54,862 INFO memory.MemoryStore: MemoryStore cleared
2020-05-08 23:34:54,871 INFO storage.BlockManager: BlockManager stopped
2020-05-08 23:34:54,884 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
2020-05-08 23:34:54,919 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2020-05-08 23:34:54,988 INFO spark.SparkContext: Successfully stopped SparkContext
2020-05-08 23:34:55,961 INFO util.ShutdownHookManager: Shutdown hook called
2020-05-08 23:34:55,964 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3af61df0-d8be-4a7c-854d-8ee3494448bf/pyspark-5c2fd809-115b-4a0b-87ad-42d27845835a
2020-05-08 23:34:55,993 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-69feafc4-e200-41b0-bf01-9e68b597cd1a
2020-05-08 23:34:55,996 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3af61df0-d8be-4a7c-854d-8ee3494448bf
